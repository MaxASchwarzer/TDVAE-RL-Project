# TD-VAE

TD-VAE implementation in PyTorch 1.0.

This code implements the ideas presented in the paper [Temporal Difference Variational Auto-Encoder (Gregor et al)][2]. This implementation includes configurable number of stochastic layers as well as the specific multilayer RNN design proposed in the paper.

**NOTE**: This implementation also makes use of [`pylego`][1], which is a minimal library to write easily extendable experimental machine learning code.

## Results

Here are the results on Moving MNIST, where the context length is 11, and the last 5 images in each row are generated by jumpy prediction from the model. This is the same setting as the one presented in the paper.
![Figure with context 11, 5 predictions](./results/11_5.png)

Here are the results on Moving MNIST, where the context length is 1, and the remaining 15 images in each row are generated by jumpy prediction from the model. This shows the model sampling a direction and jumpy transitioning through the states without having enough information in the context.
![Figure with context 1, 15 predictions](./results/1_15.png)

[1]: https://github.com/ankitkv/pylego
[2]: https://arxiv.org/abs/1806.03107

## Replication
To replicate our results:
1.  For model-free, run `python main.py --model conditional.tdvae  --name tdqvae`
2.  For the DQN baseline, run `python main.py --model conditional.drqn --rl True  --name dqn`
2.  For model-based, run `python main.py --model conditional.modeltdvae --tdvae_weight 1 --rl_weight 10 --mpc True --eps_decay_end 1 --name mpc`
